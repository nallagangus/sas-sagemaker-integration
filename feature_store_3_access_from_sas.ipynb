{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook demos retreival of training data from AWS feature store into CAS in-memory.\n",
    "### Type 1. Retreive data from feature store into a panda dataframe, save data into .csv file and then load csv file. \n",
    "### Type 2. Explore featuregroup settings to find underlying S3 data location and then load data directly.\n",
    "### Type 3. Use Hive/EP if you have hive cluster running. This probably fastest way to load data from feature store \n",
    "### Type 4. This is for online feature store record access. Ideal for fast data access during inference. It reads only the latest row(s) identified by record_identifier.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 - This sets default properties for remainder of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_bucket for everything.\n",
    "default_bucket = \"fsbu-user1-s3bucket-1\"\n",
    "\n",
    "# offline store area for feature-store. - You do not need this ..\n",
    "## feature_store_s3_bucket = \"fsbu-user1-feature-store\"\n",
    "\n",
    "# feature group we are reading and loading into CAS in-memory table. \n",
    "feature_group_name = \"fraud-summary-feature-group\"\n",
    "\n",
    "# All access defined on this role\n",
    "sagemaker_iam_role='arn:aws:iam::123456789012:role/fsbu-user1-sagemaker-fullaccess'\n",
    "\n",
    "# following are for SAS/SWAT\n",
    "cashost = \"fsds-viya34lab-controller.fsl.sashq-d.openstack.sas.com\"\n",
    "#cashost = \"fsbuviya4.fsbu-openstack-k8s.unx.sas.com\"\n",
    "casport = 5570\n",
    "protocol = \"cas\"\n",
    "username = \"user1\"\n",
    "\n",
    "# python boto3 picks creds from $home/.aws/config and credentials. \n",
    "# SWAT Viya4 operates on K8S and cannot see home directories. so we need folloowing for Viya4/K8S aws access. \n",
    "# CAS ctontroller pods should see these PVC - you know what I am talking about.\n",
    "aws_config = \"/datasourcelib/user1/aws/config\"\n",
    "aws_credentials = \"/datasourcelib/user1/aws/credentials\"\n",
    "aws_config = \"/sgrid/home/user1/.aws/config\"\n",
    "aws_credentials = \"/sgrid/home/user1/.aws/credentials\"\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - The following cell is common for all types. Basically creates a CAS connection and boto3 connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ···········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Grid node action status report: 5 nodes, 8 total actions executed.\n",
      "status :  [About]\n",
      "\n",
      " {'CAS': 'Cloud Analytic Services',\n",
      "  'Version': '3.05',\n",
      "  'VersionLong': 'V.03.05M0P11112019',\n",
      "  'Copyright': 'Copyright © 2014-2018 SAS Institute Inc. All Rights Reserved.',\n",
      "  'ServerTime': '2021-08-12T02:12:27Z',\n",
      "  'System': {'Hostname': 'fsds-viya34lab-controller.fsl.sashq-d.openstack.sas.com',\n",
      "   'OS Name': 'Linux',\n",
      "   'OS Family': 'LIN X64',\n",
      "   'OS Release': '3.10.0-1062.1.1.el7.x86_64',\n",
      "   'OS Version': '#1 SMP Fri Sep 13 22:55:44 UTC 2019',\n",
      "   'Model Number': 'x86_64',\n",
      "   'Linux Distribution': 'CentOS Linux release 7.7.1908 (Core)'},\n",
      "  'license': {'site': 'Viya 3.5 FSBU GA Shipped',\n",
      "   'siteNum': 70180938,\n",
      "   'expires': '11Feb2022:00:00:00',\n",
      "   'gracePeriod': 45,\n",
      "   'warningPeriod': 45}}\n",
      "\n",
      "[server]\n",
      "\n",
      " Server Status\n",
      " \n",
      "    nodes  actions\n",
      " 0      5        8\n",
      "\n",
      "[nodestatus]\n",
      "\n",
      " Node Status\n",
      " \n",
      "                                                 name        role  uptime  \\\n",
      " 0  fsds-viya34lab-worker-1.fsl.sashq-d.openstack....      worker   0.448   \n",
      " 1  fsds-viya34lab-worker-2.fsl.sashq-d.openstack....      worker   0.448   \n",
      " 2  fsds-viya34lab-worker-3.fsl.sashq-d.openstack....      worker   0.447   \n",
      " 3  fsds-viya34lab-worker-4.fsl.sashq-d.openstack....      worker   0.448   \n",
      " 4  fsds-viya34lab-controller.fsl.sashq-d.openstac...  controller   0.513   \n",
      " \n",
      "    running  stalled  \n",
      " 0        0        0  \n",
      " 1        0        0  \n",
      " 2        0        0  \n",
      " 3        0        0  \n",
      " 4        0        0  \n",
      "\n",
      "+ Elapsed: 0.0108s, user: 0.00591s, sys: 0.00679s, mem: 1.41mb\n"
     ]
    }
   ],
   "source": [
    "# This cell is common and required for all 3 types of data access.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "boto3_session = boto3.Session()\n",
    "s3_client = boto3_session.client('s3')\n",
    "sagemaker_client = boto3_session.client('sagemaker')\n",
    "featurestore_runtime = boto3_session.client(service_name=\"sagemaker-featurestore-runtime\")\n",
    "\n",
    "feature_store_session = Session(boto_session=boto3_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    ")\n",
    "\n",
    "fraud_summary_feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=feature_store_session)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import swat\n",
    "import pandas\n",
    "import json\n",
    "from getpass import getpass\n",
    "\n",
    "#os.environ['CAS_CLIENT_SSL_CA_LIST'] = '/sgrid/openssl_certs/cas_controller_certs/fsds-viya34lab-controller.pem'\n",
    "#os.environ['CAS_CLIENT_SSL_CA_LIST'] = '/sgrid/openssl_certs/cas_controller_certs/fsbuviya4.fsbu-openstack-k8s.unx.sas.com.pem'\n",
    "#os.environ['TKESSL_OPENSSL_LIB'] = '/usr/lib64/libssl.so.10'\n",
    "os.environ['SSLREQCERT']='ALLOW'  \n",
    "## ==> if you want to ignore cert business.\n",
    "\n",
    "password = getpass()\n",
    "#password = \"\"\n",
    "conn = swat.CAS(cashost, casport, protocol=protocol,username=username,password=password)\n",
    "out = conn.serverstatus()\n",
    "print(\"status : \", out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 1 to load data into Viya in-memory table. \n",
    "##### Retrieve data into panda data frame, keep columns you want and then save into a S3 file. Then load that S3 file into CAS-in-memory\n",
    "##### Time it takes to read data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__attrs_attrs__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_current_query_execution_id', '_result_bucket', '_result_file_prefix', 'as_dataframe', 'catalog', 'database', 'get_query_execution', 'run', 'sagemaker_session', 'table_name', 'wait']\n",
      "pd frame shape : (171428, 8)\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table TMPJUWJ8KLW in caslib CASUSERHDFS(user1).\n",
      "NOTE: The table TMPJUWJ8KLW has been created in caslib CASUSERHDFS(user1) from binary data uploaded to Cloud Analytic Services.\n",
      "[TableInfo]\n",
      "\n",
      "           Name    Rows  Columns  IndexedColumns Encoding  \\\n",
      " 0  TMPJUWJ8KLW  171427        4               0    utf-8   \n",
      " \n",
      "          CreateTimeFormatted           ModTimeFormatted  \\\n",
      " 0  2021-08-11T22:12:42-04:00  2021-08-11T22:12:42-04:00   \n",
      " \n",
      "          AccessTimeFormatted JavaCharSet    CreateTime  ...  Repeated  View  \\\n",
      " 0  2021-08-11T22:12:42-04:00        UTF8  1.944354e+09  ...         0     0   \n",
      " \n",
      "    MultiPart  SourceName  SourceCaslib  Compressed Creator Modifier  \\\n",
      " 0          0                                     0  user1            \n",
      " \n",
      "       SourceModTimeFormatted SourceModTime  \n",
      " 0  2021-08-11T22:12:42-04:00  1.944354e+09  \n",
      " \n",
      " [1 rows x 23 columns]\n",
      "\n",
      "+ Elapsed: 0.00926s, user: 0.00616s, sys: 0.00817s, mem: 1.83mb\n",
      "Type 1 - Total time for reading feature store data --- 8.813116788864136 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# read data into a panda data frame and then into a CSV and finally into CAS table. \n",
    "# Time the whole operation which means run this cell in it's entirety \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "a_query = fraud_summary_feature_group.athena_query()\n",
    "a_table = a_query.table_name\n",
    "a_query_string = ('select * from \"' + a_table + '\"' )\n",
    "a_query.run(query_string=a_query_string, output_location= \"s3://\" + default_bucket + \"/query_results\")\n",
    "a_query.wait()\n",
    "a_pd = pd.DataFrame()\n",
    "a_pd = a_query.as_dataframe()\n",
    "print(dir(a_query))\n",
    "print(\"pd frame shape :\", a_pd.shape)\n",
    "\n",
    "# save data into a CSV file\n",
    "a_pd = a_pd [\n",
    "    [\n",
    "        \"auth_account_id\",\n",
    "        \"auth_date\",\n",
    "        \"daily_sum\",\n",
    "        \"daily_count\"\n",
    "    ]\n",
    "]\n",
    "a_pd.to_csv(\"../data/temp_fraud_summary.csv\", header=False,index=False)\n",
    "\n",
    "# read CSV file into CAS memory \n",
    "tbl = conn.read_csv('../data/temp_fraud_summary.csv',casout={\"replication\":'0'})\n",
    "#print(conn.table.fileinfo(caslib='CASUSERHDFS'))\n",
    "print(conn.table.tableinfo(caslib='CASUSERHDFS'))\n",
    "\n",
    "# print the time \n",
    "print(\"Type 1 - Total time for reading feature store data --- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 2 to load data into Viya in-memory table. \n",
    "##### Retrieve feature data right of offline feature store underlying S3 folder. \n",
    "##### All data is stored as parquets.. so use recurse = true during loadTable action into CAS-in-memory\n",
    "##### Time it takes to read data. \n",
    "##### We need to able to recursively read partitioned parquet datasets.\n",
    "##### Viya 2021.1.3 does not support and so this needs to wait until that functionality is same as that of CSV files(see earlier cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket:  fsbu-user1-s3bucket-1\n",
      "key:  s3-test-folder\n",
      "ds:  {\"srctype\":\"s3\",\"bucket\":\"fsbu-user1-s3bucket-1\", \"objectPath\":\"/s3-test-folder/\", \"awsConfigPath\": '/sgrid/home/user1/.aws/config', \"awsCredentialsPath\": '/sgrid/home/user1/.aws/credentials' } \n",
      "NOTE: 'S3CASLIBx' is now the active caslib.\n",
      "NOTE: Cloud Analytic Services added the caslib 'S3CASLIBx'.\n",
      "NOTE: The file, '/s3-test-folder/sub-folder-1/sub-folder-2/test_1b.csv' was used to create the CAS Table column names.\n",
      "NOTE: The CSV file table load for table, 'sub-folder-1' produced 60 rows from 3 files.\n",
      "NOTE: Cloud Analytic Services made the file sub-folder-1 in AWS S3 bucket fsbu-user1-s3bucket-1 available as table SUB-FOLDER-1 in caslib S3CASLIBx.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>S3CASLIBx</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>SUB-FOLDER-1</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('SUB-FOLDER-1', caslib='S3CASLIBx')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.777s</span> &#183; <span class=\"cas-user\">user 0.156s</span> &#183; <span class=\"cas-sys\">sys 0.139s</span> &#183; <span class=\"cas-memory\">mem 264MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'S3CASLIBx'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'SUB-FOLDER-1'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('SUB-FOLDER-1', caslib='S3CASLIBx')\n",
       "\n",
       "+ Elapsed: 0.777s, user: 0.156s, sys: 0.139s, mem: 264mb"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TEST CELL - do not use it . This is just for testing multi-file load on CSV's \n",
    "### DO NOT RUN. IGNORE ERRORS IF ANY\n",
    "#NO NEED TO RUN THIS. JUST A TEST SETUP FOR CSVS\n",
    "\n",
    "# split bucket and relative folder \n",
    "feature_group_bucket = \"fsbu-user1-s3bucket-1\"\n",
    "feature_group_s3_prefix = \"s3-test-folder\"\n",
    "\n",
    "print(\"bucket: \", feature_group_bucket)\n",
    "print(\"key: \", feature_group_s3_prefix )\n",
    "\n",
    "datasource = \"{\\\"srctype\\\":\\\"s3\\\",\\\"bucket\\\":\\\"\" + feature_group_bucket + \"\\\", \\\"objectPath\\\":\\\"/\" + feature_group_s3_prefix + \"/\\\", \\\"awsConfigPath\\\": '\" + aws_config + \"', \\\"awsCredentialsPath\\\": '\" + aws_credentials + \"' } \" \n",
    "print (\"ds: \", datasource)\n",
    "\n",
    "# make sure subdirectories is true.\n",
    "conn.table.addCaslib(name='S3CASLIBx', description='', subDirectories='True', session='true', activeOnAdd='true', dataSource=eval(datasource), createDirectory='false')\n",
    "conn.table.loadtable(caslib='s3CASLIBx',path=\"sub-folder-1\",importOptions={\"fileType\":\"csv\",\"multiFile\":\"True\",\"recurse\":\"True\"})\n",
    "#conn.table.dropcaslib(caslib='s3caslib3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 path for feature group:  s3://fsbu-user1-feature-store/123456789012/sagemaker/us-east-1/offline-store/fraud-summary-feature-group-1628479259/data\n",
      "bucket:  fsbu-user1-feature-store\n",
      "key:  123456789012/sagemaker/us-east-1/offline-store/fraud-summary-feature-group-1628479259\n",
      "ds:  {\"srctype\":\"s3\",\"bucket\":\"fsbu-user1-feature-store\", \"objectPath\":\"/123456789012/sagemaker/us-east-1/offline-store/fraud-summary-feature-group-1628479259/\", \"awsConfigPath\": '/sgrid/home/user1/.aws/config', \"awsCredentialsPath\": '/sgrid/home/user1/.aws/credentials' } \n",
      "Type 2 - Total time for reading feature store data --- 0.20631814002990723 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Use boto3 and sagemaker sdk to get feature group settings of where offline data is saved\n",
    "# Read the data straight out of S3 using S3 parquet connector.\n",
    "# SAS Viya version (Viya4 and above) should support recursive partitioned parquet datasets.\n",
    "# Viya 2021.1.3 does not support and so this needs to wait until that functionality is same as that of CSV files(see earlier cell)\n",
    "##\n",
    "##\n",
    "# Time the whole operation which means run this cell in it's entirety \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "fraud_summary_feature_group_resolved_output_s3_uri = (\n",
    "    fraud_summary_feature_group.describe()\n",
    "    .get(\"OfflineStoreConfig\")\n",
    "    .get(\"S3StorageConfig\")\n",
    "    .get(\"ResolvedOutputS3Uri\")\n",
    ")\n",
    "print(\"S3 path for feature group: \", fraud_summary_feature_group_resolved_output_s3_uri)\n",
    "\n",
    "# split bucket and relative folder \n",
    "feature_group_bucket = fraud_summary_feature_group_resolved_output_s3_uri.split(\"/\")[2]\n",
    "feature_group_s3_prefix = fraud_summary_feature_group_resolved_output_s3_uri.replace(\n",
    "    f\"s3://{feature_group_bucket}/\", \"\"\n",
    ")\n",
    "\n",
    "# following hack to replace \"data\" at the end since SAS Viya cannot process tag path=\".\" for current directory. it needs a non-null value\n",
    "# so we replace data with blank only to access it back as path=\"data\" during load table action - sudhir reddy\n",
    "feature_group_s3_prefix = feature_group_s3_prefix.replace(f\"/data\", \"\")\n",
    "\n",
    "print(\"bucket: \", feature_group_bucket)\n",
    "print(\"key: \", feature_group_s3_prefix )\n",
    "\n",
    "datasource = \"{\\\"srctype\\\":\\\"s3\\\",\\\"bucket\\\":\\\"\" + feature_group_bucket + \"\\\", \\\"objectPath\\\":\\\"/\" + feature_group_s3_prefix + \"/\\\", \\\"awsConfigPath\\\": '\" + aws_config + \"', \\\"awsCredentialsPath\\\": '\" + aws_credentials + \"' } \" \n",
    "#datasource = \"{\\\"srctype\\\":\\\"s3\\\",\\\"bucket\\\":\\\"\" + feature_group_bucket + \"\\\", \\\"objectPath\\\":\\\"/\" + feature_group_s3_prefix + \"/\\\", \\\"awsConfigPath\\\":'/sgrid/home/user1/.aws/config', \\\"awsCredentialsPath\\\":'/sgrid/home/user1/.aws/credentials'} \" \n",
    "print (\"ds: \", datasource)\n",
    "\n",
    "# make sure subdirectories is true.\n",
    "#conn.table.addCaslib(name='S3CASLIB2', description='', subDirectories='True', session='true', activeOnAdd='true', dataSource=eval(datasource), createDirectory='false')\n",
    "#conn.table.loadtable(caslib='s3caslib2',path=\"data\",importOptions={\"fileType\":\"PARQUET\",\"multiFile\":\"True\",\"recurse\":\"True\"},casout={\"replication\":'0'})\n",
    "#conn.table.loadtable(caslib='s3caslib2',path=\"data\",importOptions={\"fileType\":\"PARQUET\"})\n",
    "#print(conn.table.tableinfo(caslib='CASUSERHDFS'))\n",
    "\n",
    "# print the time \n",
    "print(\"Type 2 - Total time for reading feature store data --- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Elapsed: 0.00927s, user: 0.00277s, sys: 0.00948s, mem: 1.42mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The caslib 's3caslib2' does not exist in this session.\n",
      "ERROR: The action stopped due to errors.\n"
     ]
    }
   ],
   "source": [
    "print(conn.table.caslibinfo(caslib='s3caslib2',verbose=\"True\"))\n",
    "#conn.table.dropcaslib(caslib='s3caslib2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 3 to load data into Viya in-memory table by leveraging EMR. \n",
    "##### feature stores use S3 for storage and during creation of feature groups (notebook1) weeeeget option to save that DDL\n",
    "##### DDL refers to S3 folder as an external table. So We will simply define that table externally outside this note book.We need to define table only one time and not for every updates/loading of feature data .\n",
    "##### Once We define that table we can load data using SAS Access to hadoop serial as well as parallel to load data.\n",
    "##### This is probably faster than first two because we can use EMR and EP parallel loading . "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# run following on EMR for only one time after feature store is created. Work with hadoop/emr admin\n",
    "create database sagemaker_featurestore ;\n",
    "\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS sagemaker_featurestore.fraud_summary_feature_group\n",
    "(auth_account_id BIGINT,\n",
    " auth_date STRING,\n",
    " daily_sum DOUBLE,\n",
    " daily_count BIGINT ,\n",
    " EventTime DOUBLE,\n",
    " write_time BIGINT,\n",
    " event_time BIGINT,\n",
    " is_deleted BOOLEAN)\n",
    " ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    " STORED AS PARQUET\n",
    " LOCATION 's3://fsbu-user1-feature-store/123456789012/sagemaker/us-east-1/offline-store/fraud-summary-feature-group-1627918092/data'\n",
    ";\n",
    "\n",
    "## see output after table is created. Matches to rows\n",
    " hive -e 'select count(*) from sagemaker_featurestore.fraud_summary_feature_group ' ;\n",
    "\n",
    "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: false\n",
    "Query ID = hadoop_20210806190940_dc52cda2-8c82-4e96-b699-70ff51ad7cfa\n",
    "Total jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Status: Running (Executing on YARN cluster with App id application_1628275487801_0013)\n",
    "\n",
    "----------------------------------------------------------------------------------------------\n",
    "        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\n",
    "----------------------------------------------------------------------------------------------\n",
    "Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0\n",
    "Reducer 2 ...... container     SUCCEEDED      1          1        0        0       0       0\n",
    "----------------------------------------------------------------------------------------------\n",
    "VERTICES: 02/02  [==========================>>] 100%  ELAPSED TIME: 68.78 s\n",
    "----------------------------------------------------------------------------------------------\n",
    "OK\n",
    "171428\n",
    "Time taken: 78.129 seconds, Fetched: 1 row(s)\n",
    "###\n",
    "\n",
    "### issues \n",
    "## be careful about type mismatches between Parquet versions created by feature store and Hive versions supported by EMR.\n",
    "## I had to change hive table column types to BIGINT, DOUBLE and INT for many.. So watch out \n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds:  {\"srctype\":\"hadoop\",\"schema\":\"sagemaker_featurestore\", \"uid\":\"hadoop\", \"hadoopConfigDir\":'/sgrid/hadoop_jars_conf/emr-5.33.0/sitexmls',\"hadoopJarPath\":'/sgrid/hadoop_jars_conf/emr-old/jars',\"server\":'localhost'} \n",
      "NOTE: 's3caslib3' is now the active caslib.\n",
      "NOTE: Cloud Analytic Services added the caslib 's3caslib3'.\n",
      "NOTE: Performing serial LoadTable action using SAS Data Connector to Hadoop.\n",
      "NOTE: Cloud Analytic Services made the external data from fraud_summary_feature_group available as table FRAUD_SUMMARY_FEATURE_GROUP in caslib s3caslib3.\n",
      "[TableInfo]\n",
      "\n",
      "                           Name    Rows  Columns  IndexedColumns Encoding  \\\n",
      " 0  FRAUD_SUMMARY_FEATURE_GROUP  171428        8               0    utf-8   \n",
      " \n",
      "          CreateTimeFormatted           ModTimeFormatted  \\\n",
      " 0  2021-08-11T22:19:59-04:00  2021-08-11T22:19:59-04:00   \n",
      " \n",
      "          AccessTimeFormatted JavaCharSet    CreateTime  ...  Repeated  View  \\\n",
      " 0  2021-08-11T22:19:59-04:00        UTF8  1.944354e+09  ...         0     0   \n",
      " \n",
      "    MultiPart                   SourceName  SourceCaslib  Compressed Creator  \\\n",
      " 0          0  fraud_summary_feature_group     s3caslib3           0  user1   \n",
      " \n",
      "   Modifier  SourceModTimeFormatted SourceModTime  \n",
      " 0                                            NaN  \n",
      " \n",
      " [1 rows x 23 columns]\n",
      "\n",
      "+ Elapsed: 0.0121s, user: 0.00696s, sys: 0.00724s, mem: 1.83mb\n",
      "Type 2 - Total time for reading feature store data --- 129.38743543624878 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hadoop_schema = \"sagemaker_featurestore\"\n",
    "hadoop_table_name = \"fraud_summary_feature_group\"\n",
    "hadoop_uid = \"hadoop\"\n",
    "data_transfeR_mode = \"serial\"\n",
    "hadoop_config_dir = \"/sgrid/hadoop_jars_conf/emr-5.33.0/sitexmls\"\n",
    "hadoop_jar_path = \"/sgrid/hadoop_jars_conf/emr-old/jars\"\n",
    "# established ssh tunnel from CAS controller to EMR Hive server else latter will not let you make it from on-prem.\n",
    "hadoop_server = \"localhost\"\n",
    "\n",
    "#dataSource={dataTransferMode='Serial', uid='hadoop', hadoopJarPath='/sgrid/hadoop_jars_conf/emr-old/jars', schema='sagemaker_featurestore', properties='hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;hive.support.concurrency=true', allFSTypes=true, srcType='hadoop', hadoopConfigDir='/sgrid/hadoop_jars_conf/emr-5.33.0/sitexmls', server='localhost'}\n",
    "\n",
    "datasource = \"{\\\"srctype\\\":\\\"hadoop\\\",\\\"schema\\\":\\\"\" + hadoop_schema + \"\\\", \\\"uid\\\":\\\"\" + hadoop_uid + \"\\\", \\\"hadoopConfigDir\\\":'\" + hadoop_config_dir + \"',\\\"hadoopJarPath\\\":'\" + hadoop_jar_path + \"',\\\"server\\\":\\'\" + hadoop_server + \"\\'} \" \n",
    "print (\"ds: \", datasource)\n",
    "\n",
    "conn.table.addCaslib(name='s3caslib3', description='', subDirectories='True', session='true', activeOnAdd='true', dataSource=eval(datasource), createDirectory='false')\n",
    "conn.table.loadtable(caslib='s3caslib3',path=hadoop_table_name,casout={\"replication\":'0'})\n",
    "print(conn.table.tableinfo(caslib='s3caslib3'))\n",
    "\n",
    "# print the time \n",
    "print(\"Type 2 - Total time for reading feature store data --- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CASLibInfo]\n",
      "\n",
      "         Name    Type Description Path  \\\n",
      " 0  s3caslib3  hadoop                    \n",
      " 1  s3caslib3                            \n",
      " 2  s3caslib3                            \n",
      " 3  s3caslib3                            \n",
      " 4  s3caslib3                            \n",
      " \n",
      "                                           Definition  Subdirs  Local  Active  \\\n",
      " 0                                     uid = 'hadoop'      1.0    1.0     1.0   \n",
      " 1  hadoopJarPath = '/sgrid/hadoop_jars_conf/emr-o...      NaN    NaN     NaN   \n",
      " 2                  schema = 'sagemaker_featurestore'      NaN    NaN     NaN   \n",
      " 3  hadoopConfigDir = '/sgrid/hadoop_jars_conf/emr...      NaN    NaN     NaN   \n",
      " 4                               server = 'localhost'      NaN    NaN     NaN   \n",
      " \n",
      "    Personal  Hidden  Transient  \n",
      " 0       0.0     0.0        0.0  \n",
      " 1       NaN     NaN        NaN  \n",
      " 2       NaN     NaN        NaN  \n",
      " 3       NaN     NaN        NaN  \n",
      " 4       NaN     NaN        NaN  \n",
      "\n",
      "+ Elapsed: 0.00953s, user: 0.00382s, sys: 0.00843s, mem: 1.81mb\n"
     ]
    }
   ],
   "source": [
    "print(conn.table.caslibinfo(caslib='s3caslib3',verbose=\"True\"))\n",
    "#conn.table.dropcaslib(caslib='s3caslib3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#out = conn.serverstatus()\n",
    "#print(\"status : \", out)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_swatsasctl",
   "language": "python",
   "name": "py36_swatsasctl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
